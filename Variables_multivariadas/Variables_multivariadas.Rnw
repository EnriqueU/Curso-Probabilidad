\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{bigints}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\title{Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle

\vspace{0.3cm}

{\Large Variables aleatorias multivariadas }


\vspace{0.5cm}

Sean un par de variables aleatorias discretas $X$ e $Y$. Considerando el par  $(X,Y)$ como un vector aleatorio con valores en $\mathbb{R}^2$, definamos el  \texttt{jmf} \texttt{(joint mass function)} de $X$ e $Y$  como la funci\'on $p_{X,Y} : \mathbb{R}^2 \rightarrow [0,1]$ definida por


\vspace{0.3cm}

\[
p_{X,Y}(x,y) = \mathbb{P}(\omega \in \Omega :X(\omega) =x , Y(\omega)=y)
\]

\vspace{0.3cm}

o de forma abreviada :

\vspace{0.3cm}

\[
p_{X,Y}(x,y) = \mathbb{P}(X=x , Y=y)
\]


\vspace{0.5cm}


Algunas propiedades que se deducen de la definici\'on:

\vspace{0.3cm}

\begin{itemize}
\item $p_{X,Y}(x,y) = 0$ a menos que $x \in \texttt{Im X}$ y $y \in \texttt{Im Y}$.
\item  $\displaystyle\sum_{x \in \texttt{Im X}}\displaystyle\sum_{y \in \texttt{Im Y}}p_{X,Y}(x,y) = 1$

\end{itemize}


\vspace{0.3cm}

\textbf{Ejemplo} Sea una distribuci\'on bivariada para dos variables aleatorias $X$ y $Y$ tomando los valores $0$ o $1 $:

\vspace{0.3cm}


\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/9   & 2/9   & 1/3 \\
X = 1 & 2/9   & 4/9   & 2/3 \\
\hline
      & 1/3   & 2/3   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

As\'i, $p(1,1) = \mathbb{P}(X = 1,y =1) = 4/9$.

\vspace{0.5cm}

Los \texttt{pmf} $p_X$ y $p_Y$ de $X$ y $Y$ pueden ser encontrados desde $p_{X,Y}$ de la siguiente manera


\vspace{0.3cm}

\begin{align} 
p_{X}(x) = \mathbb{P}(X=x) = \sum_{y \in \texttt{Im Y}}\mathbb{P}(X = x, Y=y) = \sum_{y}p_{X,Y}(x,y) 
\end{align}


\vspace{0.2cm}

de manera similar

\vspace{0.2cm}

\begin{align}
p_{Y}(y) = \mathbb{P}(Y=y) = \sum_{x \in \texttt{Im X}}\mathbb{P}(X = x, Y=y) = \sum_{x}p_{X,Y}(x,y)
\end{align}


\vspace{0.3cm}

Estas funciones son llamadas \texttt{funciones marginales} de $X$ e $Y$ respectivamente. Si vemos a $(X,Y)$ puntos aleatorios en el plano, entonces $X$ e $Y$ son la proyecci\'on de este punto sobre los ejes coordenados.

\vspace{0.3cm}
\textbf{Ejemplo} Supongamos que $X$ e $Y$ son variables aleatorias que toman los valores $1,2 $ y $3$ y la probabilidad que el par $(X,Y)$ es igual a $(x ,y)$ es dado por la tabla 

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll l}
      & x = 1 & x = 2 & x=3     \\
      \hline
y = 1 & 1/12   &3/18    & 1/6 \\
y = 2 & 1/18   & 0      & 5/18 \\
y = 3 & 0      &3/18    & 1/12
\end{tabular}
\end{table}

\vspace{0.3cm}

Entonces para este ejemplo

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(X =3) &= \mathbb{P}(X =3, Y =1) + \mathbb{P}(X =3, Y =2) + \mathbb{P}(X =3, Y =3)\\
 &= \frac{1}{6} + \frac{5}{18} + \frac{1}{12} = \frac{19}{36}
\end{align*}

\vspace{0.2cm}


De manera similar

\vspace{0.2cm}

\[
\mathbb{P}(Y = 2) = \frac{1}{18} + 0 + \frac{5}{18} = \frac{1}{3}.
\]


\vspace{0.3cm}

\textbf{Ejemplo} Supongamos que $p_{X,Y}$ es dado por la tabla siguiente. La distribuci\'on marginal para $X$  corresponde a las filas y la distribuci\'on marginal para $Y$ corresponde  a las columnas.


\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/10   & 2/10   & 3/10 \\
X = 1 & 3/10   & 4/10   & 7/10 \\
\hline
      & 4/10   & 6/10   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

Por ejemplo, $p_{X}(0) = 3/10$ y $p_{X}(1) = 7/10$.


\vspace{0.5cm}


De manera similar, se aplica a $X = (X_1, X_2, \dots,X_n)$ un vector de variables aleatorias discretas. Para este caso el \texttt{jmf} de $X$ es definido como la funci\'on $p_X$ definido por

\vspace{0.3cm}

\[
p_{X}(x) = \mathbb{P}(X_1 = x_1, X_2 = x_2, \dots, X_n= x_n)
\]

\vspace{0.3cm}

para $x = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$.


\vspace{0.3cm}


\textbf{La esperanza en el caso multivariado}

\vspace{0.3cm}

Si $X$ e $Y$ son variables aleatorias discretas y sea $g : \mathbb{R}^2 \rightarrow \mathbb{R}$ entonces $Z = g(X,Y)$ es una variable aleatoria tambi\'en, definida formalmente como $Z(\omega) = g(X(\omega),Y(\omega))$ para $\omega \in \mathbb{R}$. La esperanza de $Z$ puede ser calculada directamente desde la funci\'on $p_{X,Y}(x,y) = \mathbb{P}(X = x, Y =y)$ como lo indica el siguiente teorema

\vspace{0.3cm}

\textbf{Teorema 1} Tenemos que

\vspace{0.2cm}

\[
\mathbb{E}(g(X,Y)) = \displaystyle\sum_{x \in \texttt{Im X}}\displaystyle\sum_{y \in \texttt{Im Y}}g(x ,y)\mathbb{P}(X =x, Y=y)
\]

\vspace{0.2cm}

siempre que esta suma converga absolutamente.

\vspace{0.3cm}

Una consecuencia importantes de este teorema, es que el operador $\mathbb{E}$ actua de manera lineal sobre el conjunto de las variables discretas. Es decir si $X$ e $Y$ son variables aleatorias discretas y $a, b \in \mathbb{R}$, entonces

\vspace{0.3cm}

\[
\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)
\]

\vspace{0.3cm}

siempre que $\mathbb{E}(X)$ y  $\mathbb{E}(Y)$ existan. 


\vspace{0.5cm}

\textbf{Independencia de variables aleatorias discretas }

\vspace{0.3cm}

Dos variables aleatorias discretas $X$ e $Y$ son independientes si el par de eventos $\{X =x \}$ y $\{Y =y\}$ son independientes para todo $x, y \in \mathbb{R}$ y se escribe esta condici\'on como 

\vspace{0.3cm}

\[
\mathbb{P}(X =x, Y = y) = \mathbb{P}(X =x)\mathbb{P}(Y =y) \ \ \ \text{para}\ \ x, y \in \mathbb{R}.
\]

\vspace{0.2cm}

Las variables aleatorias que no son independientes, se llaman \texttt{dependientes}.

\vspace{0.3cm}

\textbf{Teorema 2} Las variables aleatorias  discretas $X$ e $Y$ son independientes si y s\'olo si existen funciones $f,g : \mathbb{R} \rightarrow \mathbb{R}$ tal que se cumple que el \texttt{jmf} de $X$ e $Y$ satisface

\vspace{0.3cm}

\begin{align}
p_{X,Y}(x,y) = f(x)g(y)\ \ \ \text{para} \ \ x, y \in \mathbb{R}.
\end{align}


\vspace{0.3cm}

Probemos que si se cumple la condici\'on (3) para las funciones $f$ y $g$ y $x \in \texttt{Im X}$ y $y \in \texttt{Im Y}$ tenemos 

\vspace{0.3cm}

\[
p_{X}(x) = f(x)\displaystyle\sum_{y}g(y), \ \ p_{Y}(y) = g(y)\displaystyle\sum_{x}f(x)
\]

\vspace{0.3cm}

por propiedad

\vspace{0.3cm}

\begin{align*}
1 = \displaystyle\sum_{x,y}p_{X,Y}(x,y) &= \displaystyle\sum_{x,y}f(x)g(y)\\
&= \displaystyle\sum_{x}f(x)\displaystyle\sum_{y}g(y)
\end{align*}

\vspace{0.3cm}

Por tanto 

\vspace{0.3cm}

\begin{align*}
p_{X,Y}(x,y) &= f(x)g(y) =f(x)g(y)\displaystyle\sum_{x}f(x)\displaystyle\sum_{y}g(y)\\
&= p_{X}(x)p_{Y}(y)
\end{align*}

\vspace{0.3cm}

\textbf{Teorema 3} Si $X$ e $Y$ son variables aleatorias discretas independientes con esperanza $\mathbb{E}(X)$ y $\mathbb{E}(Y)$ entonces

\vspace{0.2cm}

\[
\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)
\]

\vspace{0.3cm}

Por el teorema 1:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(XY) &= \displaystyle\sum_{x, y}xy\mathbb{P}(X = x, Y =y) \\
&= \displaystyle\sum_{x,y}xy\mathbb{P}(X =x)\mathbb{P}(Y =y)\\
&= \displaystyle\sum_{x}x\mathbb{P}(X = x)\displaystyle\sum_{y}y\mathbb{P}(Y = y) = \mathbb{E}(X)\mathbb{E}(Y)
\end{align*}

\vspace{0.3cm}

El inverso de este teorema es falso si $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$ entonces $X$ e $Y$ son independientes como indica el siguiente ejemplo

\vspace{0.3cm}


\textbf{Ejemplo} Supongamos que $X$ tiene una distribuci\'on dada por

\vspace{0.2cm}

\[
\mathbb{P}(X = -1) = \mathbb{P}(X = 0) = \mathbb{P}(X =1) = \frac{1}{3}
\]

\vspace{0.2cm}

y $Y$ es dado por

\vspace{0.2cm}

\[
Y = \begin{cases}
 0 & \text{si} \ \ X = 0\\
 1 & \text{si} \ \ X \neq 0
\end{cases}
\]

\vspace{0.3cm}

Si tomamos $\Sigma = \{-1, 0, 1 \}$ y $\mathbb{P}$ dado por $\mathbb{P}(-1) = \mathbb{P}(0) = \mathbb{P}(1) = \frac{1}{3} $ y sea $X(\omega) = \omega, Y(\omega) = \vert \omega \vert$. Entonces $X$ e $Y$ son dependientes:

\vspace{0.3cm}


\[
\mathbb{P}(X = 0, Y = 1) = 0
\]

\vspace{0.3cm}

Pero

\vspace{0.3cm}

\[
\mathbb{P}(X = 0)\mathbb{P}(Y =1) = \frac{1}{3}\cdot\frac{2}{3} = \frac{2}{9}
\]

\vspace{0.3cm}

Por otro lado:

\vspace{0.3cm}

\begin{align*}
\mathbb{E}(XY) = \displaystyle\sum_{x, y}xy\mathbb{P}(X =x, Y =y)\\
= (-1)\cdot\frac{1}{3} + 0\cdot\frac{1}{3} + 1\cdot\frac{1}{3} = 0
\end{align*}

\vspace{0.3cm}

\[
\mathbb{E}(X)\mathbb{E}(Y) = 0\cdot\frac{2}{3} = 0
\]


\vspace{0.5cm}


\textbf{Teorema 4} Las variables aleatorias $X$ e $Y$ son independientes si y s\'olo si

\vspace{0.3cm}

\[
\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))
\]

\vspace{0.3cm}

para todas las funciones $g,h: \mathbb{R} \rightarrow \mathbb{R}$ en el cual las esperanzas existen.

\vspace{0.5cm}

Para el caso $X = (X_1, X_2, \dots, X_n)$ de variables aleatorias con $n > 2$. Por ejemplo la familia $X$ es llamada independiente si

\vspace{0.2cm}

\[
\mathbb{P}(X_1 =x_1, \dots, X_n =x_n) = \mathbb{P}(X_1 =x_1)\cdots\mathbb{P}(X_n =x_n)
\]

\vspace{0.2cm}

Adem\'as $X_1, X_2, \dots, X_n$ son independientes

\vspace{0.2cm}

\[
\mathbb{E}(X_1, X_2, \cdots, X_n) = \mathbb{E}(X_1)\mathbb{E}(X_2)\cdots \mathbb{E}(X_n)
\]

\vspace{0.2cm}

La familia $X$ es llamada \texttt{independiente componente a componente} si $X_i$ y $Y_j$ son independientes si $i \leq j$.
}


\vspace{0.5cm}

\textbf{Suma de variables aleatorias}

\vspace{0.2cm}

Si $X$ e $Y$ son variables aleatorias discretas, con un \texttt{jmf}.?` Cu\'al es la funci\'on masa de $Z = X + Y$?. Para este caso $Z$ toma valores de $z$ si y s\'olo si $X =x$ y $Y = z -x$ para alg\'un $x$, entonces

\vspace{0.3cm}


\begin{align*}
\mathbb{P}(Z =z) &= \mathbb{P}\Bigl(\bigcup_{x}(\{X =x \} \cap \{Y = z -x \}) \Bigr)\\
 &= \displaystyle\sum_{x \in \texttt{Im X}}\mathbb{P}(X = x, Y = z -x) \ \ \text{para}\ \ z \in \mathbb{R}
\end{align*}

\vspace{0.3cm}


\textbf{F\'ormula de Convoluci\'on} Si $X$ e $Y$ son variables aleatorias discretas independientes, entonces $Z = X +Y$ tiene un funci\'on masa

\vspace{0.3cm}

\[
\mathbb{P}(Z =z) = \displaystyle\sum_{x \in \texttt{Im X}}\mathbb{P}(X =x)\mathbb{P}(Y = z -x)\ \ \text{para}\ \ z \in \mathbb{R}.
\]

\vspace{0.3cm}


La f\'ormula de convoluci\'on dice que la funci\'on de masa de $X + Y$ es la \texttt{convoluci\'on} de la funci\'on de $X$ e $Y$.


\vspace{0.3cm}

\textbf{Funci\'on indicador} La funci\'on indicador de un evento $A$, denotado por $1_A$ y es dado por

\vspace{0.3cm}

\[
1_{A}(\omega) = \begin{cases}

1 & \text{si}\ \ \omega \in A \\
0 & \text{si}\ \  \omega \notin A
\end{cases}
\]

\vspace{0.3cm}

La funci\'on $1_A$ indica si o no ocurre $A$. Para esta variable aleatoria discreta, la esperanza es dada por

\vspace{0.2cm}

\[
\mathbb{E}(1_A) = \mathbb{P}(A)
\]


\vspace{0.3cm}

Algunas propiedades de la funci\'on indicador

\vspace{0.2cm}

\begin{itemize}
\item $1_{A \cap B} = 1_A1_B$
\item $1_A + 1_{A^c} = 1$
\end{itemize}

\vspace{0.3cm}

\textbf{Ejemplo} Sean $A_1, A_2, \dots, A_n$ eventos y $1_{A_i}$ la funci\'on indicador de $A_i$.

\vspace{0.2cm}

Por propiedad, la uni\'on $A = A_1 \cup A_2 \dots, A_n$ tiene como funci\'on indicador

\vspace{0.3cm}

\[
1_A = 1 - \prod_{i =1}^{n}(1 -1_{A_i})
\]

\vspace{0.3cm}

El producto puede ser expandido y agrupado para obtener

\vspace{0.3cm}

\begin{align*}
1_A = \displaystyle\sum_{i}1_{A_i} -  \displaystyle\sum_{i < j}1_{A_i}1_{A_j} +  \displaystyle\sum_{i < j < k}1_{A_i}1_{A_j}1_{A_k} - \cdots \\
+ (-1)^{n +1}1_{A_1}1_{A_2}\cdots 1_{A_n}
\end{align*}

\vspace{0.3cm}

Tomando esperanzas, obtenemos la \texttt{f\'ormula de inclusi\'on-exclusi\'on}

\vspace{0.3cm}

\begin{align*}
\mathbb{P}\Bigl(\bigcup_{i}A_i \Bigr) = \displaystyle\sum_{i}\mathbb{P}(A_i) - \displaystyle\sum_{i < j}\mathbb{P}(A_i \cap A_j) + \displaystyle\sum_{i< j <k}\mathbb{P}(A_i\cap A_j\cap A_k) - \cdots \\
\cdots + (-1)^{n +1}\mathbb{P}\Bigl(\bigcap_{i}A_i\Bigr)
\end{align*}


\vspace{0.8cm}


En el caso de variables aleatorias en general,  estudiamos el \texttt{jdf (joint distribution function)} del par X, Y de variables aleatorias que se define como la funci\'on $F_{X,Y}: \mathbb{R}^2 \rightarrow [0,1]$ es dado por

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = \mathbb{P}(X \leq x, Y \leq y)
\]

\vspace{0.3cm}

Algunas propiedades importantes de \texttt{jfd} es

\vspace{0.2cm}

\begin{align*}
\lim_{x,y \rightarrow -\infty}F_{X,Y}(x,y) = 0 \\
\lim_{x,y \rightarrow \infty}F_{X,Y}(x,y) = 1
\end{align*}

\vspace{0.2cm}

\begin{itemize}
\item $F_{X,Y}(x_1, y_1) \leq F_{X,Y}(x_, y_2)$ si $x_1 \leq x_2$ y $y_1 \leq y_2$.
\item Se define las funciones marginales de $F_{X,Y}$ como  $F_X(x) = \lim_{y \rightarrow \infty}F_{X,Y}(x,y)$ y  $F_Y(y) = \lim_{x \rightarrow \infty}F_{X,Y}(x,y)$.
\end{itemize}

\vspace{0.3cm}

Las variables aleatorias $X$ e $Y$ son independientes, si para todos los $x,y \in \mathbb{R}$, los eventos $\{ X \leq x\}$ y $\{Y \leq y \}$ son independientes. Es decir $X$ e $Y$ son independientes si y s\'olo si

\vspace{0.3cm}

\[
\mathbb{P}(X \leq x, Y \leq y) = \mathbb{P}(X \leq x)\mathbb{P}(Y\leq y) \ \ \text{para}\ \ x, y \in \mathbb{R}.
\]

\vspace{0.3cm}

lo que dice que el \texttt{jdf} se factoriza como el producto de las dos funciones de distribuci\'on marginal

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\ \ \text{si}\ \ x,y \in \mathbb{R}.
\]

\vspace{0.3cm}

Las variables aleatorias que no son independientes, se llaman dependientes.

\vspace{0.3cm}

En general, si $X_1, X_2, \dots X_n$ son variables aleatorias, el \texttt{jdf}  es la funci\'on $F_X:\mathbb{R}^n\rightarrow [0,1]$ es dado por

\vspace{0.3cm}

\[
F_{X}(x) = \mathbb{P}(X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n)
\]

\vspace{0.3cm}

para $x = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$. La variables $X_1, X_2, \dots, X_n$ son independientes

\vspace{0.3cm}

\[
\mathbb{P}(X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n) = \mathbb{P}(X_1 \leq x_1)\cdots \mathbb{P}(X_n \leq x_n)
\]

\vspace{0.3cm}

o equivalentemente

\vspace{0.3cm}

\[
F_{X}(x) = F_{X_1}(x_1)\cdots F_{X_n}(x_n)\ \ \text{si}\ \ x\in \mathbb{R}.
\]


\vspace{0.3cm}

\textbf{Ejemplo} Sean $X$ e $Y$ dos variables aleatorias, cada una tomando los valores $\{\dots, -1,0, 1, \dots \}$ con \texttt{jmf}


\vspace{0.3cm}

\[
\mathbb{P}(X =i, Y= j) = p(i,j)\ \ \text{para}\ \ i,j =0,\pm 1, \pm 2, \dots
\]

\vspace{0.3cm}

El \texttt{jdf} es dado por

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = \displaystyle\sum_{i\leq x, j\leq y}p(i,j)\ \ \text{para}\ \ (x, y) \in \mathbb{R}^2
\]


\vspace{0.3cm}

\textbf{Ejemplo} Sean $X$ e $Y$ son variables aleatorias con un \texttt{jdf}

\[
F_{X,Y}(x,y) = \begin{cases}
1 -e^{-x} - e^{-y} + e^{x +y} & \text{si}\ \ x, y \geq 0\\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.3cm}

La funci\'on de distribuci\'on marginal de $X$ es

\vspace{0.3cm}

\[
F_{X}(x) = \lim_{y \rightarrow \infty}F_{X,Y}(x,y) = \begin{cases}
1 -e^{-x} & \text{si}\ \ x \geq 0 \\
0 & \text{en otro casos}
\end{cases}.
\]

\vspace{0.3cm}

as\'i $X$ tiene una distribuci\'on exponencial con par\'ametro $1$. Un c\'alculo muestra que $Y$ tiene esta distribuci\'on tambi\'en. As\'i

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\ \ \text{para}\ \ x,y \in \mathbb{R}.
\]

\vspace{0.3cm}

por tanto $X$ e $Y$ son independientes.


\vspace{0.8cm}


El par $(X,Y)$ de variables aleatorias es llamada \textbf{continua conjunta } si su \texttt{jdf} es expresable de la forma

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = \mathbb{P}(X \leq x, Y \leq y) = \bigintsss_{u=-\infty}^{x} \bigintsss_{v=-\infty}^{y}f(u,v)dudv
\]


\vspace{0.3cm}

para $x, y \in \mathbb{R}$ para alguna funci\'on $f:\mathbb{R}^2 \rightarrow [0, \infty)$. Si esto se cumple, decimos que $X$ e $Y$ tienen \texttt{pdf} conjunta $f$ que usualmente se identifica como $f_{X,Y}$.

\vspace{0.3cm}

Si $X$ e $Y$ son continuas conjuntas, podemos tener  el \texttt{pdf}  de la siguiente manera

\vspace{0.3cm}

\[
f_{X,Y}(x,y) = \begin{cases}
\dfrac{\partial ^2}{\partial x \partial y}F_{X,Y}(x,y) & \text{si esta derivada existe en $(x, y)$} \\
0 & \texttt{en otros casos}
\end{cases}
\]


\vspace{0.3cm}


Algunas propiedades elementales de $f_{X,Y}$, son

\vspace{0.2cm}

\begin{itemize}
\item $f(x,y) \geq 0$ para todo $(x,y) \in \mathbb{R}$.
\item $\bigintsss_{\infty}^{\infty}\bigintss_{\infty}^{\infty}f(x,y)dxdy = 1$ y
\end{itemize}
.
\vspace{0.3cm}

\textbf{Ejemplo} Sea $(X,Y)$ uniforma sobre el cuadrado unitario. Entonces


\vspace{0.2cm}

\[
f(x,y) = \begin{cases}
1 & \text{si}\ \ 0 \leq x \leq 1, 0 \leq y \leq 1 \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

Encontremos $\mathbb{P}(X < 1/2, Y < 1/2)$.

\vspace{0.2cm}

El evento $A = \{ X < 1/2, Y < 1/2 \}$ corresponde a un subconjunto del cuadrado unidad. Integrando $f$ sobre este conjunto, corresponde a calcular el \'area de conjunto $A$ que es $1/4$. As\'i $\mathbb{P}(X < 1/2, Y < 1/2) = 1/4$.

\textbf{Ejemplo} Sea $(X,Y)$ que tienen una funci\'on de densidad

\vspace{0.2cm}

\[
f(x,y)= \begin{cases}
cx^2y & \mbox{si}\ \  x^2 \leq y \leq 1\\
0 & \mbox{ en otro caso}.
\end{cases}
\]

\vspace{0.2cm}

Notemos primero que $-1 \leq x \leq 1$. Encontremos el valor de $c$, para ello fijemos un valor $x$ y dejemos que $y$ varie en su rango, el cu\'al es $x^2 \leq y \leq 1$.

\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.45]{m1.png}
\end{figure}
As\'i

\begin{align*}
1 &= \int\int f(x,y)dxdy = c\int_{-1}^{1}\int_{x^2}^{1}x^2ydydx \\
&= c\int_{-1}^{1}x^2\Bigl[\int_{x^2}^{1}ydy\Bigr]dx = c \int_{-1}^{1}x^2\frac{1 -x^4}{2}dx = \frac{4c}{21}.
\end{align*}


\vspace{0.2cm}


Ahora sea $c = 21/4$. Calculemos $\mathbb{P}(X \geq Y)$.

\vspace{0.2cm}

Esto corresponde al conjunto $A = \{ (x,y); 0 \leq x \leq 1, x^2 \leq y \leq x\}$. (Se puede ver esto, en el diagrama de arriba). As\'i


\begin{align*}
\mathbb{P}(X \geq Y) &= \frac{21}{4}\int_{0}^{1}\int_{x^2}^{x}x^2ydydx = \frac{21}{4}\int_{0}^{1}x^2\Bigl[\int_{x^2}^{x}ydy\Bigr]dx\\\ &= \frac{21}{4}\int_{0}^{1}x^2\Bigl( \frac{x^2 - x^4}{2}\Bigr)dx = \frac{3}{20}.
\end{align*}


\vspace{0.5cm}

Para el par $(x,y) \in \mathbb{R}$ y para $\delta x$ y $\delta y$, la probabilidad que el vector aleatorio $(X,Y)$ est\'a en el peque\~no rect\'angulo con los puntos $(x,y)$ en el extremo izquierdo y de lados $\delta x$ y $\delta y$ es

\vspace{0.3cm}

\[
\mathbb{P}(x < X \leq x + \delta x, y < Y \leq y + \delta y) \approx f_{X,Y}(x,y)\delta x\delta y
\]

\vspace{0.3cm}

Este resultado conduce al siguiente teorema

\vspace{0.3cm}

\textbf{Teorema 5} Si $A$ es un subconjunto \textbf{regular} de $\mathbb{R}^2$ y $X$ e $Y$ son variables aleatorias continuas conjuntas con funci\'on densidad $f_{X,Y}$ entonces

\vspace{0.3cm}

\[
\mathbb{P}((X,Y) \in A) = \bigintss \bigintsss_{(x,y) \in A}f_{X,Y}(x,y)dxdy
\]

\vspace{0.3cm}

\textbf{Ejemplo}La funci\'on $f$  definida por

\vspace{0.3cm}

\[
f(x,y) = \begin{cases}
\frac{1}{ab} & \text{si}\ \ 0 < x < b, 0 < y < b \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.3cm}

es una funci\'on densidad conjunta. Si $X$ e $Y$ tienen como densidad $f$, el vector $(X,Y)$ se dice que es \texttt{uniformemente distribuido} sobre el rect\'angulo $B= (0,a) \times (0,b)$. Para alguna regi\'on del plano

\vspace{0.3cm}

\begin{align*}
\mathbb{P}((X,Y) \in A) &= \bigintsss\bigintsss_{A}f(x,y)dxdy \\
&= \bigintsss\bigintsss_{A \cap B}f(x,y)dxdy = \dfrac{\text{area}(A \cap B)}{\text{area}(B)}
\end{align*}

\vspace{0.5cm}

Siempre que el par $X, Y$ tiene  un \texttt{jdf} $f_{X,Y}$ , las densidades marginales de $X$ e $Y$ en los puntos de diferenciabilidad, son 

\vspace{0.2cm}

\begin{align}
f_{X}(x) = \bigintsss_{v = -\infty}^{\infty} f_{X,Y}(x,v)dv \ \ f_{Y}(y) = \bigintsss_{u = -\infty}^{\infty} f_{X,Y}(u,y)du
\end{align}

\vspace{0.2cm}

Las correspondientes funciones de distribuci\'on marginal, de $X$ e $Y$ son denotadas por $F_X$ y $F_{Y}$ y son las proyecciones del vector aleatorio $(X,Y)$ sobre los ejes coordenados. 

\vspace{0.5cm}

\textbf{Ejemplo} Sean $(X,Y)$ con una funci\'on densidad
\[
f(x,y) = \begin{cases}
\frac{21}{4}x^2y & \mbox{si}\ x^2 \leq y \leq 1 \\
0 & \mbox{ en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

As\'i

\vspace{0.2cm}


\[
f_{X}(x) = \int f(x,y)dy = \frac{21}{4}x^2\int_{x^2}^{1}ydy = \frac{21}{8}x^2(1 - x^4)
\]

\vspace{0.2cm}

para $-1 \leq x \leq 1$ y $f_{X}(x) = 0$ en otros casos.

\vspace{0.8cm}


Las variables aleatorias $X$ e $Y$ son independientes si su funci\'on de distribuci\'on satisface

\vspace{0.3cm}

\[
F_{X,Y}(x ,y) = F_{X}(x)F_{Y}(y) \ \ \text{para}\ \ x, y \in \mathbb{R}
\]


\vspace{0.5cm}
 
 Si $X$ e $Y$ son continuas conjuntas, entonces diferenciando esta relaci\'on con respecto $x$ e $y$ se cumple

\vspace{0.3cm}

\[
f_{X,Y}(x ,y) = f_{X}(x)f_{Y}(y) \ \ \text{para}\ \ x, y \in \mathbb{R}
\]

\vspace{0.5cm}


\textbf{Ejemplo} Sean $X$ y $Y$ tienen la siguiente distribuci\'on

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/4   & 1/4   & 1/2 \\
X = 1 & 1/4   & 1/4   & 1/2 \\
\hline
      & 1/2   & 1/2   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

Entonces, $f_{X} (0) = f_{X}(1) = 1/2$ y $f_{Y}(0) = f_{Y}(1) = 1/2$. $X$ y $Y$ son independientes, ya que $f_{X}(0)f_{Y}(0) = f(0,0), f_{X}(0)f_{Y}(1) = f(0,1),  f_{X}(1)f_{Y}(0) = f(1,0), f_{X}(1)f_{Y}(1) = f(1,1)$. Pero si $X$ y $Y$ tienen la misma distribuci\'on

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/2   & 0    & 1/2 \\
X = 1 & 0     & 1/2   & 1/2 \\
\hline
      & 1/2   & 1/2   &  1  
\end{tabular}
\end{table}

\vspace{0.3cm}

ellas no son independientes, ya que $f_{X}(0)f_{Y}(1) = (1/2)(1/2) = 1/4$, pero $f(0,1) = 0$.

\vspace{0.5cm}

\textbf{Ejemplo} Supongamos que $X$ y $Y$ son independientes y tienen la misma densidad

\[
f(x) = \begin{cases}
2x & \mbox{si}\  \ 0 \leq x \leq 1\\
0  & \mbox{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

Encontremos $\mathbb{P}(X + Y \leq 1)$. Usando independencia, tenemos que  la \texttt{jdf} es

\[
f(x,y) = f_{X}(x)f_{Y}(y) = \begin{cases}
4xy & \mbox{si}\ \  0 \leq x \leq 1,\ \ 0 \leq y \leq 1\\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.5cm}

Ahora,

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(X +  Y \leq 1) &= \bigintsss \bigintsss_{x + y \leq 1}f(x,y)dydx \\
&= 4\bigintsss_{0}^{1}x\Bigl[\bigintsss_{0}^{1-x}ydy \Bigr]dx \\
&=4 \bigintsss_{0}^{1}x\frac{(1 - x)^2}{2}dx = \frac{1}{6}.
\end{align*}

\vspace{0.5cm}

\textbf{Teorema 6}  Supongamos que  $X$ y $Y$ son continuas conjuntas, son independientes si y s\'olo si el \texttt{jdf} puede ser expresado en la forma

\vspace{0.3cm}

\[
f_{X, Y}(x,y) = g(x)h(y) \ \text{para} \ \ x, y \in \mathbb{R}
\]

\vspace{0.3cm}

\textbf{Ejemplo} Sean $X$ y $Y$ con densidad

\vspace{0.3cm}

\[
f(x,y) =\begin{cases}
2e^{-(x + 2y)} & \mbox{si} \ \ x > 0 \ \ y \ \ y > 0\\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

El rango de $X$ e $Y$ es el rect\'angulo $(0, \infty) \times (0, \infty)$. Podemos escribir $f(x, y) = g(x)h(y)$, donde $g(x) = 2e^{-x}$ y $h(y) = e^{-2y}$. As\'i, $X$ y $Y$ son independientes.

\vspace{0.8cm}

\textbf{Suma de variables aleatorias continuas}

\vspace{0.3cm}

Supongamos que $X$ e $Y$ tienen una densidad $f_{X,Y}$, entonces la densidad de $Z = X +Y$ se puede calcular

\vspace{0.5cm}

\begin{align*}
\mathbb{P}(Z \leq z) = \mathbb{P}(X + Y \leq z)\\
= \bigintsss\bigintsss_{A}f_{X,Y}(x,y)dxdy
\end{align*}

\vspace{0.3cm}

por el \texttt{teorema 5} donde $A = \{(x,y)\in \mathbb{R}^2: x +y \leq z \}$. Escribiendo en los l\'imites de integraci\'on encontramos 

\vspace{0.3cm}


\begin{align*}
\mathbb{P}(Z \leq z) &=\bigintsss_{x = -\infty}^{\infty}\bigintsss_{y = -\infty}^{z -x}f_{X,Y}(x,y)dxdy\\
&= \bigintsss_{v = -\infty}^{z}\bigintsss_{u = -\infty}^{\infty}f_{X,Y}(u,v -u)dudv
\end{align*}

\vspace{0.3cm}

con la sustituci\'on $u = x, v = x + y$. Diferenciando esta ecuaci\'on con respecto a $z$ , se tiene

\vspace{0.3cm}

\[
f_{Z}(z) = \bigintsss_{-\infty}^{\infty}f_{X,Y}(u, z -u)du
\]


\vspace{0.3cm}

Un caso importante es cuando $X$ e $Y$ son independientes, como indica el siguiente teorema

\vspace{0.3cm}

\textbf{Teorema 7} Si las variables aleatorias $X$ e $Y $ son independientes y  continuas con funciones densidad $f_X$ y $f_Y$, entonces la funci\'on densidad de $f_{X,Y}$ de $Z = X + Y$ es

\[
f_{Z}(z) = \bigintsss_{-\infty}^{\infty}f_{X}(x)f_{Y}(z - x)dx \ \ \text{para}\ \ z \in \mathbb{R}.
\]


\vspace{0.5cm}

\textbf{Ejemplo} Si $X$ e $Y$ son variables aleatorias independientes, con la distribuci\'on gamma con param\'etros $s$ y $\lambda$ y la distribuci\'on gamma param\'etros $t$ y $\lambda$. Entonces $Z = X +Y$ tiene una funci\'on densidad

\vspace{0.3cm}

\begin{align*}
f_{Z}(z) &= \bigintsss_{\infty}^{\infty}f_{X}(x)f_{Y}(z -x)\\
&= \begin{dcases}
\bigintsss_{0}^{z}f_{X}(x)f_{Y}(z -x)&\ \ \text{si}\ \ z > 0 \\
0 & \text{ en otros casos}
\end{dcases}
\end{align*}


\vspace{0.3cm}


desde que $f_{X}(x)f_{Y}(z -x) = 0$, sin incluir el caso $x > 0$ y $z -x > 0$. Para $z > 0$

\vspace{0.3cm}

\begin{align*}
f_{Z}(z) &= \bigintsss_{0}^{z}\dfrac{1}{\Gamma(z)}\lambda(\lambda x)^{s -1}e^{-\lambda x}\dfrac{1}{\Gamma(t)}\lambda[\lambda(z -x)]^{t -1}e^{-\lambda(z -x)}dx \\
&=Ae^{-\lambda z}\bigintsss_{0}^{z}x^{s -1}(z -x)^{t -1}dx
\end{align*}

\vspace{0.3cm}

donde 


\[
A = \dfrac{1}{\Gamma(s)\Gamma(t)}\lambda^{s + t}
\]

\vspace{0.3cm}

Sustituyendo $y = z/x$ en la \'ultima integral obtenemos

\vspace{0.3cm}

\[
f_{Z}(z) =Bz^{s +t -1}e^{-\lambda z} \ \ \text{si} \ \ z > 0
\]

\vspace{0.3cm}

donde $B$ es una constante dado por :

\begin{align*}
B = \dfrac{1}{\Gamma(s)\Gamma(t)}\lambda^{s +1}\bigintsss_{0}^{1}y^{s -1}(1 -y)^{t -1}dy
\end{align*}

\vspace{0.3cm}

La \'unica distribuci\'on con funci\'on densidad en este c\'alculo es la distribuci\'on gamma con par\'ametros $s +t$ y $\lambda$ y se sigue que la constante $B$ satisface

\vspace{0.3cm}

\[
B = \dfrac{1}{\Gamma(s +t)}\lambda^{s +t}
\]


\vspace{0.3cm}

Luego, la suma de dos variables aleatorias independientes con un distribuci\'on gamma  con par\'ametros $s, \lambda$ y $t, \lambda$ respectivamente, tiene una distribuci\'on gamma con par\'ametros $s + t, \lambda$.  Comparando las dos \'ultimas ecuaciones tenemos

\vspace{0.3cm}

\[
\bigintsss_{0}^{1}y^{s -1}(1 -y)^{t -1}dy = \dfrac{\Gamma(s)\Gamma(t)}{\Gamma(s + t)}\ \ \text{para}\ \ s, t> 0
\]

\vspace{0.8cm}

\textbf{Cambio de variables}

\vspace{0.3cm}

Si $X$ e $Y$ son variables aleatorias y $u, v: \mathbb{R}^2 \rightarrow \mathbb{R}$. El par $(U,V)$ de variables aleatorias dadas por $U = u(X,Y)$ y $V = v(x,y)$ tiene una distribuci\'on que se puede calcular de la siguiente manera:

\vspace{0.3cm}

Sea $T$ una funci\'on $\mathbb{R}^2 \rightarrow \mathbb{R}$ dado por $T(x, y) = (u,v)$, donde $u = u(x,y)$ y $v = v(x,y)$, suponiendo que $T$ es una biyecci\'on entre alg\'un dominio $D \subseteq \mathbb{R}^2$ y alg\'un $S \subseteq \mathbb{R}^2$, entonces $T$ puede ser invertido y obtener una biyecci\'on $T^{-1}: S \rightarrow D$. Esto es, para cada $(u ,v) \in S$, existe un punto $(x,y) = T^{-1}(u,v)$ en $D$ con  $x = x(u,v)$ y $y = y(u,v)$. 
\vspace{0.2cm}

El Jacobiano de $T^{-1}$ es definido como el determinante de


\vspace{0.3cm}

\[
J(u,v) = \begin{vmatrix}
\dfrac{\partial x}{\partial u}&\dfrac{\partial x}{\partial v}\\
\dfrac{\partial x}{\partial v}& \dfrac{\partial y}{\partial v}
\end{vmatrix} = \dfrac{\partial x}{\partial u}\dfrac{\partial y}{\partial v} - \dfrac{\partial y}{\partial u}\dfrac{\partial x}{\partial v}
\]


\vspace{0.3cm}

suponiendo que esas derivadas existen y son continuas en todos los puntos de $S$.

\vspace{0.3cm}

Entonces si $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$, para un conjunto \texttt{regular} $A$ de $D$ y alguna funci\'on integrable $g$

\vspace{0.3cm}

\begin{align}
\bigintsss\bigintsss_{A}g(x, y)dxdy = \bigintsss\bigintsss_{T(A)}g\bigl(x(u,v), y(u,v)\bigr)\vert J(u,v)\vert dudv.
\end{align}

\vspace{0.2cm}

donde $T(A)$ es la imagen de $A$ bajo $T$.


\vspace{0.5cm}

\textbf{Teorema 8} Sean $X$ e $Y$ son continuas conjuntas con densidad $f_{X,Y}$ y sea $D = \{(x, y): f_{X,Y}(x, y) > 0 \}$. Si la aplicaci\'on $T$ es dado por $T(x ,y) = (u(x,y), v(x,y))$ es una biyecci\'on desde $D$ al conjunto $S \subseteq \mathbb{R}^2$, entonces el par $(U,V) = (u(X,Y), v(X,Y))$ es continua conjunta con una densidad

\vspace{0.3cm}

\begin{align*}
f_{U,V}(u,v) = \begin{dcases}
f_{X,Y}\bigl(x(u,v), y(u,v)\bigr)\vert J(u,v)\vert & \ \ \text{si}\ \ (u,v) \in S, \\
0 & \text{en otros casos}
\end{dcases}
\end{align*}

\vspace{0.3cm}

Supongamos que $A \subseteq D$ y que $T(A) = B$. Desde que $T: D \rightarrow S$ es una biyecci\'on,

\vspace{0.3cm}

\begin{align}
\mathbb{P}\bigl((U,V)\in B \bigr) = \mathbb{P}\bigl((X,Y)\in A \bigr)
\end{align}


\vspace{0.5cm}

Sin embargo

\vspace{0.3cm}

\begin{align*}
 \mathbb{P}\bigl((X,Y)\in A \bigr) &= \bigintsss\bigintsss_{A}f_{X,Y}(x,y)dxdy \\
 &= \bigintsss\bigintsss_{B}f_{X,Y}\bigl(x(u,v), y(u,v)\bigr)\vert J(u,v)\vert dudv  &\ \ \  \text{por (5)} \\
 &= \mathbb{P}\bigl((U,V)\in B \bigr)& \ \ \ \text{por (6)}
\end{align*}

\vspace{0.3cm}

Esto se cumple para un $B \subseteq S$ y por el \texttt{teorema 5} se tiene el resultado pedido.

\vspace{0.5cm}

\textbf{Ejemplo} Sean $(X, Y)$ con una funci\'on densidad

\vspace{0.3cm}

\[
f(x, y) = \begin{cases}
e^{ -x -y } & \text{si}\ \ x ,y > 0, \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.3cm}

y $U = X + Y$ y $V = X/(X + Y)$. Encontremos la densidad de $U$ y  $V$ y la funci\'on marginal de $V$.


\vspace{0.5cm}


La funci\'on $T$ para este caso es dado por: $T(x ,y) = (u, v)$, donde  

\vspace{0.3cm}


\[
u = x + y, \ \ \ v = \dfrac{x}{x + y}
\]

\vspace{0.3cm}

y $T$ es una biyecci\'on desde $D = \{(x,y): x, y >0 \}$ a $S = \{(u,v): 0 < u < \infty,  0 < v < 1 \}$, lo que produce una funci\'on inversa $T^{-1}(u ,v) = (x ,y)$ donde

\vspace{0.3cm}

\[
x = uv, \ \ \ y = u(1 -v)
\]

\vspace{0.3cm}

El Jacobiano de $T^{-1}$ es

\vspace{0.3cm}

\[
J(u,v) = \begin{vmatrix}
\dfrac{\partial x}{\partial u}&\dfrac{\partial x}{\partial v}\\
\dfrac{\partial x}{\partial v}& \dfrac{\partial y}{\partial v}
\end{vmatrix} = \begin{vmatrix}
v & u \\
(1 -v) & -u
\end{vmatrix} = -u
\]

\vspace{0.3cm}

y por el \texttt{teorema 8}, $U$ y $V$ tienen una funci\'on densidad

\vspace{0.3cm}

\[
f_{U,V}(u ,v) = \begin{cases}
ue^{-u} & \text{si}\ \ u > 0 \ \text{y}\ \ 0 < v < 1 \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}


La funci\'on marginal de $V$ es 

\vspace{0.3cm}

\begin{align*}
f_{V}(u) &= \bigintsss_{-\infty}^{\infty}f_{U,V}(u,v)du \\
&=\begin{dcases}
\bigintsss_{0}^{\infty}ue^{-u}du = 1 & \text{si}\ \ 0 < v < 1 \\
0 & \text{en otros casos}.
\end{dcases}
\end{align*}


\vspace{0.3cm}

as\'i $V$ es uniformemente distribuida por $(0, 1)$. Se puede probar que $U$ tiene una distribuci\'on gamma con par\'ametros $2$ y $1$.


\vspace{0.8cm}

\textbf{Distribuciones condicionales}

\vspace{0.3cm}


Si $X$ y $Y$ son discretas, entonces podemos calcular la distribuci\'on condicional de $X$ dado que hemos observado $Y = y$. Especificamente, $\mathbb{P}(X =x| Y=y) = \mathbb{P}(X = x,Y=y)/\mathbb{P}(Y=y)$. Esto conduce a la \mbox{definici\'on} de \texttt{funci\'on de masa de probabilidad  condicional (Conditional probability mass function)}, que se define como 

\vspace{0.3cm}

\[
p_{Y|X}(y|x) = \mathbb{P}(Y = y| X=x) = \frac{\mathbb{P}(Y = y, X=x)}{\mathbb{P}(X = x)} = \frac{p_{X,Y}(x,y)}{p_{Y}(y)}
\]

\vspace{0.2cm}

si se cumple que  $p_{X}(x) > 0$.

\vspace{0.2cm}


Para distribuciones continuas, usamos la misma definici\'on. La interpretaci\'on difiere: en el caso discreto, $p_{X|Y}(x|y)$ es  $\mathbb{P}(Y \leq y| X=x) $, pero cuando calculamos este resultado en el caso continuo, estamos condicionando el evento $\{X = x \}$ que tiene probabilidad $0$, por ello utilizamos la densidad y la integral para conseguir probabilidad. 

\vspace{0.5cm}

En efecto en vez,  de condicionar el evento $X = x$, condicionamos el evento $x \leq  X \leq x +\delta$ y tomando l\'imite $\delta x \rightarrow 0^{+}$. As\'i

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(Y \leq y \vert x \leq  X \leq x +\delta) &= \dfrac{\mathbb{P}(Y \leq y,  x \leq  X \leq x +\delta)}{\mathbb{P}( x \leq  X \leq x +\delta)}\\
& = \dfrac{\bigintsss_{u = x}^{x + \delta x} \bigintsss_{v = -\infty}^{y}f_{X,Y}(u,v)dudv}{\bigintsss_{x}^{x + \delta x}f_X(u) du}
\end{align*}


\vspace{0.3cm}

Dividiendo el numerador y el denominados entre $\delta x$ y tomando l\'imite cuando $\delta x \rightarrow 0^{+}$, obtenemos

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(Y \leq y \vert x \leq  X \leq x +\delta) & \rightarrow \bigintsss_{-\infty}^{y}\dfrac{f_{X,Y}(x,y)}{f_{X}(x)}dv \\
& = G(y)
\end{align*}

\vspace{0.3cm}

$G$ es una funci\'on de distribuci\'on con una funci\'on densidad

\[
g(y) = \dfrac{f_{X,Y}(x,y)}{f_{X}(x)}\ \ \ \ \text{para}\ y \in \mathbb{R}.
\]


\vspace{0.3cm}


Llamamos  a $G$ y $g$ la \texttt{funci\'on de distribuci\'on condicional} y la \texttt{funci\'on densidad condicional} de $Y$ dado que $X$ es igual a $x$, para valores de $x$ tal que $f_{X}(x) > 0$.

\vspace {0.5cm}

La \texttt{funci\'on densidad condicional } de $Y$ dado que $X =x$, denotada por $f_{Y\vert X}(\cdot \vert x)$ es definida como

\vspace{0.3cm}

\[
f_{Y|X}(y|x) = \dfrac{f_{X|Y}(x,y)}{f_{X}(x)}
\]

\vspace{0.3cm}

para $y \in \mathbb{R}$ y $x$ que cumplen que  $f_{X}(x) > 0$.


\vspace{0.3cm}

$\mathbb{P}(Y \leq y \vert X = x)$ es definida como la funci\'on distribuci\'on condicional $G(y)$ de $Y$ dado $X =x$.


\vspace{0.3cm}

Si $X$ e $Y$ son independientes, entonces $f_{X,Y}(x ,y) = f_{X}(x)f_{Y}(y)$. Por definici\'on $f_{Y\vert X}(y\vert x) = f_{Y}(y)$, lo que dice que es irrelevante $X$, cuando se estudia $Y$.

\vspace{0.5cm}

\textbf{Ejemplo} Sean $X$ y $Y$, que tienen una distribuci\'on uniforme sobre un cuadrado unidad. As\'i $f_{X|Y}(x|y) = 1$, para $0 \leq x \leq 1$ y $0$ en otros casos. Dado $Y =y$, $X$ tienen una distribuci\'on uniforme en $(0.1)$. Podemos escribir esto como $X|Y = y \sim \mbox{Uniform}(0,1)$.

\vspace{0.5cm}

\textbf{Ejemplo} Supongamos que $X \sim \mbox{Uniform}(0,1)$. Despu\'es de obtener un valor de $X$, generamos $Y|X = x \sim \mbox{Uniform}(x,1)$. La distribuci\'on marginal de $Y$ se puede calcular a partir de

\vspace{0.3cm}

\[
f_{X}(x) = \begin{cases}
1 & \mbox{si}\ \ 0 \leq x \leq 1\\
0 & \mbox{ en otros casos}.
\end{cases}
\]

luego por definici\'on tenemos

\[
f_{Y|X}(y|x) = \begin{cases}
\dfrac{1}{1 -x}& \mbox{si}\ \ 0 < x < y < 1 \\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

As\'i 

\[
f_{X,Y}(x,y) = f_{Y|X}(y|x)f_{X}(x) = \begin{cases}
\dfrac{1}{1 -x}& \mbox{si}\ \ 0 < x < y < 1 \\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

La distribuci\'on marginal para $Y$ es

\[
f_{Y}(y) = \int_{0}^{y}f_{X,Y}(x,y)dx = \int_{0}^{y}\dfrac{dx}{1 -x} = -\int_{1}^{1- y}\dfrac{du}{u} = -\log(1-y)
\]

\vspace{0.3cm}

para $0 < y < 1$.


\vspace{0.5cm}

\textbf{Ejemplo} Sea 

\vspace{0.3cm}

\[
f(x, y) = \begin{cases}
x + y & \text{si} \ \ 0 \leq x \leq 1,  0 \leq y \leq 1 \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

Encontremos $\mathbb{P}(X < 1/4 \vert Y = 1/3)$. Como $f_{Y}(y) = y + (1/2)$. As\'i

\vspace{0.3cm}

\[
f_{X\vert Y}(x\vert y) = \dfrac{f_{X,Y}(x,y)}{f_{Y}(y)} = \dfrac{x + y}{y + \frac{1}{2}}
\]

\vspace{0.3cm}

As\'i

\vspace{0.3cm}

\begin{align*}
\mathbb{P}\Bigl(X < \frac{1}{4}\Bigr\vert Y = \frac{1}{3} \Bigr) & = \bigintsss_{0}^{1/4}f_{X\vert Y}\Bigl(x \  \Bigl\vert \ \frac{1}{3}dx \Bigr) \\
& = \bigintsss_{0}^{1/4}\dfrac{x + \frac{1}{3}}{\frac{1}{3} + \frac{1}{2}} = \dfrac{\frac{1}{32} + \frac{1}{12}}{\frac{1}{3} + \frac{1}{2}} = \dfrac{11}{80}.
\end{align*}


\vspace{0.5cm}

\textbf{Ejemplo} Sean $(X,Y)$ con una funci\'on densidad

\vspace{0.3cm}

\[
f(x, y) = \begin{cases}
2e^{-x -y} & \text{si}\ \  0 < x < y < \infty \\
0  & \text{en otros casos}
\end{cases}
\]

\vspace{0.3cm}

Las funciones marginales son

\vspace{0.3cm}

\begin{align*}
f_{X}(x) &= 2e^{-2x}  \hspace{0.8cm}  \text{para}\  x >0 \\
f_{Y}(y) &= 2e^{-y}(1- e^{-y})\ \ \  \text{para}\  y > 0
\end{align*}


\vspace{0.3cm}

La funci\'on densidad condicional de $Y$ dado $X = x(>0)$ es

\vspace{0.2cm}

\[
f_{Y\vert X}(y \vert x) = \dfrac{2e^{-x - y}}{2e^{-2x}} = e^{x -y }\ \ \text{para}\ \ y > x
\]

\vspace{0.3cm}

La funci\'on densidad condicional de $X$ dado $Y = y$ es

\vspace{0.2cm}


\[
f_{X\vert Y}(x \vert y) = \dfrac{e^{-x}}{1 - e^{-y}} \ \ \text{para}\ \ 0 < x < y.
\]

\vspace{0.2cm}

En ambos casos, la funciones densidades son iguales a $0$ si $x > y$.


\vspace{0.5cm}


\textbf{Esperanza de variables aleatorias multivariadas}

\vspace{0.5cm}

Sean $X$ e $Y$ variables aleatorias continuas conjuntas y sea $g:\mathbb{R}^2 \rightarrow \mathbb{R}$. La funci\'on $Z: \Omega \rightarrow \mathbb{R}$, definido por un $Z(\omega) = g(X(\omega), Y(\omega))$ es una variable aleatoria.

\vspace{0.2cm}

En el c\'alculo  de la esperanza de $Z$, no tenemos que encontrar la distribuci\'on de $Z$ expl\'icitamente


\vspace{0.3cm}

\textbf{Teorema 9} Se tiene que

\vspace{0.3cm}

\[
\mathbb{E}(g(X, Y)) = \bigintsss_{-\infty}^{\infty}\bigintsss_{-\infty}^{\infty}g(x,y)f_{X,Y}(x, y)dxdy
\]

\vspace{0.3cm}

siempre que la integral converga absolutamente.

\vspace{0.5cm}


Usando el \texttt{teorema 9} encontramos que el operador esperanza actua linealmente sobre el espacio de las variables aleatorias continuas, es decir 

\vspace{0.3cm}

\[
\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)
\]

\vspace{0.3cm}

donde $a,b \in \mathbb{R}$ y $X$ e $Y$ son variables aleatorias conjuntas con esperanzas $\mathbb{E}(X)$ e $\mathbb{E}(Y)$.


\vspace{0.5cm}

\textbf{Teorema 10} Sean $X$ e $Y$ variables aleatorias continuas conjuntas son indepenedientes sy s\'olo si 

\vspace{0.3cm}

\[
\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X)\mathbb{E}(h(Y))
\]

\vspace{0.3cm}

para todas las funciones $g, h: \mathbb{R} \rightarrow \mathbb{R}$, para las cuales esas esperanzas existen.


\vspace{0.5cm}

En el  caso de esperanza condicional, supongamos que $X$ e $Y $ son variables aleatorias continuas conjuntas, con densidad $f_{X,Y}$ y que adem\'as que $X = x$, la densidad de $Y$ es la funci\'on densidad condicional $f_{Y\vert X}(\cdot \vert x)$.

\vspace{0.3cm}

La \texttt{esperanza condicional de $Y$ dado $X = x$} escrita como $\mathbb{E}(Y \vert X=x)$ es de la media de la funci\'on densidad condicional 

\vspace{0.3cm}

\[
\mathbb{E}(Y \vert X =x) = \bigintsss_{-\infty}^{\infty}yf_{X\vert Y}(y \vert x)dy = \bigintsss_{-\infty}^{\infty}y\dfrac{f_{X,Y}(x, y)}{f_{X}(x)}dy,
\]

\vspace{0.3cm}

v\'alido para alg\'un valor de $x$ que cumple $f_{X}(x) > 0$.

\vspace{0.5cm}

\textbf{Teorema 11} Si $X$  e $Y$ son variables aleatorias continuas conjuntas, entonces

\vspace{0.3cm}

\[
\mathbb{E}(Y) = \bigintsss\mathbb{E}(Y \vert X =x)f_{X}(x)dx
\]

\vspace{0.3cm}

donde la integral est\'a definida para los valores de $x$, tal que $f_{X}(x) > 0$.


\vspace{0.3cm}


En efecto

\vspace{0.3cm}

\begin{align*}
\mathbb{E}(Y) &= \bigintsss yf_{Y}(y)dy = \bigintsss\bigintsss yf_{X,Y}(x, y)dxdy \\
&= \bigintsss\bigintsss yf_{Y \vert X}(y \vert x)f_{X}(x)dxdy \\
&= \bigintsss \Bigl(\bigintsss yf_{Y\vert X}(y \vert x)dy \Bigr)f_{X}(x)dx
\end{align*}


\vspace{0.3cm}


Desde que $\mathbb{E}(Y \vert X)$ es una variable aleatoria, tiene una media $\mathbb{E}[\mathbb{E}(Y \vert X)]$ que puede ser calculado usando las siguientes propiedades

\vspace{0.5cm}


\begin{align*}
\mathbb{E}[\mathbb{E}(Y \vert X)] = \begin{dcases}
\displaystyle\sum_{x}\mathbb{E}(Y\vert X = x)p_{X}(x)  & \ \ \text{X discreta}\\
\bigintsss_{-\infty}^{\infty}\mathbb{E}(Y\vert X = x)f_{X}(x)  & \ \ \text{X continua}.
\end{dcases}
\end{align*}

\vspace{0.5cm}

\textbf{Teorema 12} Este resultado  llamado \texttt{ley de iteraci\'on de esperanzas }  se cumple para toda variable aleatoria  

\vspace{0.3cm}

\[
\mathbb{E}[\mathbb{E}(Y \vert X)] = \mathbb{E}(Y) \ \ \text{y} \ \ \mathbb{E}[\mathbb{E}(X \vert Y)] = \mathbb{E}(X)
\]


\vspace{0.3cm}


De manera m\'as general, para alguna funci\'on $r(X, Y)$, tenemos

\vspace{0.3cm}

\[
\mathbb{E}[\mathbb{E}(r(X,Y) \vert X)] = \mathbb{E}(r(X,Y)).
\]


\vspace{0.3cm}

Probaremos la primera ecuaci\'on

\vspace{0.3cm}

\begin{align*}
\mathbb{E}(\mathbb{E}(Y \vert X)) = \bigintsss\mathbb{E}(Y \vert X =x)f_{X}(x)dx &= \bigintsss\bigintsss yf(y \vert x)dyf(x)dx \\
&= \bigintsss\bigintsss yf(y \vert x)f(x)dxdy = \bigintsss\bigintsss yf(x, y)dxdy = \mathbb{E}(Y) 
\end{align*}

\vspace{0.5cm}

\textbf{Ejemplo} Sea una barra de longitud $l$. Dividimos la barra en un punto que se elige al azar y de manera uniforme en su longitud, y mantenemos la pieza que contiene el extremo izquierdo de la barra. A continuaci\'on, repetimos el mismo proceso con la pieza  que nos qued\'o. ?`Cu\'al es la esperanza de la longitud  prevista de las piezas que  quedan  despu\'es de romper dos veces la pieza?

\vspace{0.3cm}

Sea $Y$ la longitud de la pieza de la barra, despu\'es de que se rompe por primera vez. Sea $X$ la longitud despu\'es de que se rompe la barra por segunda vez. Asi tenemos $\mathbb{E}(X \vert Y) = Y/2$, desde que el punto de ruptura es escogido de manera uniforme sobre una pieza de longitud $Y$. De manera similar, tenemos que $\mathbb{E}(Y) = l/2$. As\'i

\vspace{0.3cm}

\[
\mathbb{E}(X) = \mathbb[\mathbb(X \vert Y)] = \mathbb{E}(Y/2) = \dfrac{\mathbb{E}(Y)}{2} = \frac{l}{4}.
\]


\vspace{0.5cm}

La varianza condicional es definida como

\vspace{0.3cm}

\[
\mathbb{V}(Y \vert X) = \bigintsss(y - \mu(x))^2f(y \vert x)dy
\]

\vspace{0.3cm}

donde $\mu(x) = \mathbb{E}(Y \vert X=x)$


\vspace{0.3cm}

\textbf{Teorema 13} Para dos variables aleatorias $X$ e $Y$

\vspace{0.3cm}

\[
\mathbb{V}(Y) = \mathbb{E}\mathbb{V}(Y \vert X) + \mathbb{V}\mathbb{E}(Y \vert X).
\]




\vspace{0.8cm}


En general, para un \texttt{vector aleatorio}, $X = (X_1,\dots, X_n)$ donde $X_1, \dots, X_n$ son variables aleatorias, es posible definir las funciones marginales, condicionales  etc,  de la misma manera que las distribuciones bivariadas.  En este caso se denota la densidad del vector aleatorio como $f(x_1,\dots,x_n)$.


\vspace{0.3cm}


Decimos que $X_1,\dots, X_n$ son independientes si, para cada $A_1, \dots, A_n$

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(X_1 \in A_1,  \dots, X_n \in A_n) = \prod_{i=1}^{n}\mathbb{P}(X_i \in A_i).
\end{align*}

\vspace{0.3cm}

Es suficiente verificar que $f(x_1,\dots, x_n) = \prod_{i = 1}^{n}f_{X_{i}}(x_i) $.


\vspace{0.5cm}

Si $X_1,\dots X_n$ son independientes y cada uno de ellos tiene la misma distribuci\'on marginal con  una funci\'on de distribuci\'on cumulativa $F$, decimos que $X_1, \dots, X_n$ son id\'enticamente distribuidas e independientes y escribimos como

\vspace{0.3cm}

\[
X_1, \dots, X_n \sim F
\]

\vspace{0.3cm}

Si $F$ tiene densidad $f$, escribimos tambi\'en $X_1, \dots, X_n \sim f$. 


\vspace{0.8cm}


\textbf{Dos importantes distribuciones multivariadas}

\vspace{0.5cm}

\textbf{1. Multinomial} La versi\'on multivariada de la distribuci\'on binomial, es llamada \texttt{multinomial}.

\vspace{0.2cm}

En efecto, considere la probabilidad de extraer una bola de una urna que contiene bolas con $k$ colores, etiquetados como \texttt{'color 1', 'color 2', \dots 'color k'}. 

\vspace{0.2cm}

Sea $p = (p1, \dots, p_k)$ donde $p_j \geq 0$ y $\sum_{j = 1}^{k}p_j = 1$ y suponiendo  que $p_j$ es la probabilidad de encontrar una bola de color $j$. Lanzamos $n$ veces (independientes lanzamientos con reemplazamiento) y sea $X = (X_1, \dots, X_k)$ donde $X_j$ es el n\'umero de veces que el color $j$ aparece. Entonces para $n = \sum_{j= 1}^{k}X_j$, decimos que $X$ tiene una distribuci\'on multinomial $\mbox{multinomial}(n,p)$. 

\vspace{0.2cm}


La funci\'on probabilidad es


\vspace{0.3cm}


\begin{align*}
f(x) = \binom{n}{x_1\dots x_k}p_{1}^{x_1}\cdots p_{k}^{x_k}
\end{align*}

\vspace{0.2cm}

donde 

\vspace{0.2cm}

\begin{align*}
\binom{n}{x_1\dots x_k} = \frac{n!}{x_1!\cdots x_k!}.
\end{align*}

\vspace{0.2cm}

Una propiedad importantes es que si   que $X \sim \mbox{multinomial(n, p)}$, donde $X = (X_1,\dots, X_k$) y $p = (p_1, \dots, p_k)$. La distribuci\'on marginal de $X_j$ es $\mbox{binomial}(n, p_j)$.


\vspace{0.5cm}

\textbf{2. Normal multivariada} La distribuci\'on normal  tiene dos par\'ametros  $\mu$ y $\sigma$. En la versi\'on multivariada, $\mu$ es un vector y $\sigma$ es reemplazada por una matriz $\Sigma$. Para empezar


\vspace{0.3cm}

\[
Z = \begin{pmatrix}
Z_1 \\
\vdots\\
Z_k
\end{pmatrix}
\]

\vspace{0.3cm}

donde $Z_1, \dots, Z_k \sim N(0,1)$ son independientes. La densidad de $Z$ es


\vspace{0.3cm}

\begin{align*}
f(z) = \prod_{i = 1}^{k}f(z_i) = \frac{1}{(2\pi)^{k/2}}\exp\Bigl\{-\frac{1}{2}\sum_{j = 1}^{k}z_j^{2}\Bigr\}\\
= \frac{1}{(2\pi)^{k/2}}\exp\Bigl\{-\frac{1}{2}z^{T}z\Bigr\}.
\end{align*}

\vspace{0.3cm}

Decimos que $Z$ tiene una distribuci\'on multivariada est\'andar escrita como $Z \sim N(0,I)$, donde $0$ representa el vector de $K$ ceros y $I$ es la matriz identidad  de orden $k \times k$. 

\vspace{0.3cm}

De manera m\'as general, un vector $X$ tiene una distribuci\'on Normal multivariada, denotada por $X \sim N(\mu, \Sigma)$, si tiene una densidad

\vspace{0.3cm}

\begin{align*}
f(x; \mu, \Sigma) = \frac{1}{(2\pi)^{k/2}\vert (\Sigma)\vert^{1/2}}\exp \Bigl\{-\frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu) \Bigr\}
\end{align*}

\vspace{0.3cm}


donde $\vert \Sigma \vert$, denota el determinante de $\Sigma$, $\mu$ es un vector de longitud $k$ y $\Sigma$ es una matriz sim\'etrica, definida positiva. 

\vspace{0.2cm}

Escribiendo $\mu = 0$ y $\Sigma = I$ tenemos la distribuci\'on Normal est\'andar.

\vspace{0.3cm}

Desde que  $\vert \Sigma \vert$ es sim\'etrica y definida positiva, existe una matriz $\Sigma^{1/2}$ llamada la ra\'iz cuadrada de $\Sigma$ con las siguientes propiedades:

\vspace{0.3cm}

\begin{enumerate}
\item  La matriz $\Sigma^{1/2}$ es sim\'etrica.
\item $\Sigma = \Sigma^{1/2}\Sigma^{1/2}$.
\item $\Sigma^{1/2}\Sigma^{-1/2} = \Sigma^{-1/2}\Sigma^{1/2} = I$ donde $\Sigma^{-1/2} = (\Sigma^{1/2})^{-1}$
\end{enumerate}

\vspace{0.5cm}

\textbf{Teorema 14} Si $Z \sim N(0,I)$ y $X = \mu + \Sigma^{1/2}Z$ entonces $X \sim N(\mu, \Sigma)$. De otro lado, si $X \sim N(\mu, \Sigma)$ entonces $\Sigma^{-1/2}(X -\mu) \sim N(0,I)$.


\vspace{0.5cm}

Supongamos que particionamos un vector normal aleatorio $X$, como $X = (X_a, X_b)$. De manera similar tenemos la partici\'on $\mu = (\mu_a, \mu_b)$ y

\vspace{0.3cm}


\[
\Sigma = \begin{pmatrix}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{pmatrix}
\]


\vspace{0.5cm}

\textbf{Teorema 15} Sea $X \sim N(\mu, \Sigma)$. Entonces

\vspace{0.3cm}

\begin{enumerate}
\item La distribuci\'on marginal de $X_a$ es $X_a \sim N(\mu_a, \Sigma_{aa})$.
\item La distribuci\'on condicional de $X_b$ dado $X_a = x_a$ es

\vspace{0.2cm}

\[
X_b|X_a = x_a \sim N(\mu_b + \Sigma_{ba}\Sigma_{aa}^{-1}(x_a - \mu_a), \Sigma_{bb} -\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}).
\]
\item Si $a$ es un vector entonces $a^{T}X \sim N(a^{T}\mu, a^{T}\sum a)$.
\item $V = (X -\mu)^{T}\Sigma^{-1}(X - \mu) \sim \chi_{k}^{2}$.
\end{enumerate}

\vspace{0.8cm}


\textbf{Transformaci\'on de  variables aleatorias}

\vspace{0.5cm}

Supongamos que $X$ es una variable aleatoria con una funci\'on densidad $f_X$ y una distribuci\'on  $F_X$ . Sea $Y = r(X)$ una funci\'on de $X$, llamada una \textbf{transformaci\'on de X}. Calculamos el  \texttt{pdf}  y el  \texttt{cdf} de $Y$ en el caso discreto,

\vspace{0.3cm}

\begin{align*}
p_{Y}(y) &= \mathbb{P}(Y=y) = \mathbb{P}(r(X) =y)\\
& = \mathbb{P}(\{x; r(x) = y\}) = \mathbb{P}(X \in r^{-1}(y))
\end{align*}

\vspace{0.3cm}

\textbf{Ejemplo} Supogamos que $\mathbb{P}(X = -1) = \mathbb{P}(X = 1) = 1/4$ y $\mathbb{P}(X = 0) = 1/2$. Sea $Y = X^2$. Entonces, $\mathbb{P}(Y = 0) = \mathbb{P}(X = 0) = 1/2$ y $\mathbb{P}(Y = 1) = \mathbb{P}(X = 1) + \mathbb{P}(X = -1) = 1/2$. Eso se resume en las siguientes tablas


\vspace{0.5cm}



\begin{minipage}{2.5in}
%\begin{table}[h]
\centering
\begin{tabular}{l| l }
$x$ & $f_{X}(x)$ \\
\hline
-1     & 1/4   \\                     
0     & 1/2   \\                
1     & 1/4                        
\end{tabular}
%\end{table} 
\end{minipage}
\begin{minipage}{2.5in}
%\begin{table}[h]
\centering
\begin{tabular}{l| l}
y & $f_Y(y)$ \\
\hline
0 & 1/2                 \\
1 & 1/2                 
\end{tabular}
%\end{table}
\end{minipage}

\vspace{0.5cm}

$Y$ toma menos valores que $X$ ya que la transformaci\'on no es uno a uno.


\vspace{0.5cm}


El caso es continuo, es m\'as dificil. Aqu\'i tres pasos para encontrar $f_Y$:

\vspace{0.3cm}

\begin{enumerate}
\item Para cada $y$, encuentra el conjunto $A_y =  \{x: r(x) \leq y \}$.
\item Encuentra  el  \texttt{cdf}

\vspace{0.3cm}

\begin{align*}
F_{Y}(y) = \mathbb{P}(Y \leq y) &= \mathbb{P}(r(X) \leq y)\\
   &= \mathbb{P}(\{x : r(x) \leq y \})\\
   &= \int_{A_y}f_{X}(x)dx 
\end{align*}
\item El \texttt{pdf} es $f_{Y}(y) = F_{Y}^{'}(y)$.
\end{enumerate}


\vspace{0.5cm}

\textbf{Ejemplo} Sea $f_{X}(x) = e^{-x}$ para $x > 0$. As\'i, $F_{X}(x) = \bigintsss_{0}^{x}f_{X}(s)ds = 1 -e^{-x}$. Sea $Y = r(X) = \log(X)$. Entonces $A_y = \{x: x\leq e^{y} \}$ y

\vspace{0.3cm}

\begin{align*}
F_{Y}(y) &= \mathbb{P}(Y\leq y) = \mathbb{P}(\log X \leq Y)\\
&=  \mathbb{P}(X \leq e^y) = F_{X}(e^y) =  1- e^{-e^{y}}.
\end{align*}

\vspace{0.3cm}

Por tanto, $f_{Y}(y) = e^ye^{-e^{y}}$ para $y \in \mathbb{R}$.


\vspace{0.5cm}


cuando $r$ es estrictamente creciente o esctrictamente decreciente, entonces $r$ tiene una inversa $s = r^{-1}$ y en este caso se cumple

\vspace{0.3cm}

\[
f_{Y}(y) = f_{X}(s(y))\Bigl\vert \dfrac{ds(y)}{dy} \Bigr\vert
\]


\vspace{0.8cm}

\textbf{Transformacion para variables aleatorias multivariadas}

\vspace{0.3cm}

Si tenemos por ejemplo, $X$ y $Y$ variables aleatorias, veamos como conocer la distribuci\'on de $X/Y, X +Y, \max\{X,Y \} $ o $\min\{X,Y \}$. Sea $Z = r(X,Y)$ una funci\'on en particular, los pasos para encontrar $f_{Z}$ son los mismo que antes

\vspace{0.3cm}

\begin{enumerate}
\item Para cada $z$, encuentra el conjunto $A_z =  \{(x,y): r(x,y) \leq z \}$.
\item Encontramos el \texttt{cdf}

\vspace{0.2cm}

\begin{align*}
F_{Z}(z) = \mathbb{P}(Z \leq z) = \mathbb{P}(r(X,Y) \leq z)\\
   = \mathbb{P}(\{(x,y) : r(x,y) \leq z \})\\
   = \bigintsss \bigintsss_{A_z}f_{X,Y}(x,y)dxdy 
\end{align*}
\item El \texttt{pdf} es $f_{Z}(z) = F_{Z}^{'}(z)$.
\end{enumerate}

\vspace{0.3cm}

\textbf{Ejemplo} Sea $X_1, X_2 \sim \mbox{Uniform}(0, 1)$ E  independientes. Encontremos la densidad de $Y = X_1 + X_2$. El  \texttt{jdf} de $(X_1,X_2)$ es

\vspace{0.2cm}

\[
f(x_1, x_2) = \begin{cases}
1 & 0< x_1 < 1, 0 < x_2 < 1\\
0 & \mbox{en otros casos}
\end{cases}
\]


\vspace{0.3cm}

Sea $r(x_1, x_2) = x_1 + x_2$. Ahora

\vspace{0.3cm}

\begin{align*}
F_{Y}(y) &= \mathbb{P}(Y \leq y)= \mathbb{P}(r(X_1,X_2) \leq y)\\
& = \mathbb{P}( \{(x_1, x_2): r(x_1, x_2)\leq y \}) = \bigintsss \bigintsss_{A_y}f(x_1, x_2)dx_1dx_2.
\end{align*}

\vspace{0.2cm}

Para encontrar $A_y$, suponemos primero que $0 < y \leq 1$. Entonces $A_y$ es el tri\'angulo con v\'ertices $(0,0), (y,0)$ y $(0.y)$. En este caso $\bigintsss \bigintsss_{A_y}f(x_1, x_2)dx_1dx_2$ es el \'area del tri\'angulo, es decir $y^2/2$. Si $1 < y < 2$, entonces $A_y$ es todo el cuadrado unitario, menos el tri\'angulo con v\'ertices $(1, y-1),(1,1), (y -1,1)$. Este conjunto tiene \'area $1 - (2 -y)^2/2$ 

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.65]{m2.png}
%\caption{Modelo del problema}
\end{figure}

\vspace{0.2cm}

Por tanto

\vspace{0.2cm}

\[
F_{Y}(y) = \begin{cases}
0 & y < 0\\
\frac{y^2}{2} & 0 \leq y < 1 \\
1 -\frac{(2 -y)^2}{2} & 1 \leq y < 2\\
1 & y \geq 2
\end{cases}
\]

\vspace{0.2cm}

Por diferenciaci\'on tenemos \texttt{cdf} es

\vspace{0.3cm}

\[
f_{Y}(y)=\begin{cases}
y & 0 \leq y \leq 1 \\
2 -y & 1\leq y \leq 2\\
0 & \mbox{ en otros casos}.
\end{cases}
\]
\end{document}
